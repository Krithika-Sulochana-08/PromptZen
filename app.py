import streamlit as st
import json
import pandas as pd
from datetime import datetime
from evaluate_prompt import evaluate_prompt, improve_prompt, compare_prompts

st.set_page_config(page_title="Prompt Quality Checker")

# Initialize prompt history if not set
if "history" not in st.session_state:
    st.session_state.history = []

st.title("üéØ Prompt Quality Checker")
st.write("Paste a prompt below and get a score from our AI expert.")

prompt = st.text_area("‚úçÔ∏è Enter your prompt here:")

if st.button("Check Quality"):
    if not prompt.strip():
        st.warning("‚ùó Please enter a prompt.")
    elif len(prompt.strip()) < 5 or not any(char.isalpha() for char in prompt):
        st.warning("‚ö†Ô∏è This prompt looks too short or may be gibberish. Please enter a more meaningful one.")
    else:
        with st.spinner("Evaluating..."):
            result = evaluate_prompt(prompt)
        try:
            data = result

            st.success("‚úÖ Prompt evaluated!")

            st.subheader("üìä Scores")
            for key in ["clarity", "specificity", "context", "creativity", "relevance", "ambiguity"]:
                st.write(f"**{key.capitalize()}**: {data[key]} / 5")

            st.markdown(f"### üßÆ **Total Score:** {data['total_score']} / 25")

            st.subheader("üîç Issues Found")
            st.write(data["issues"])

            st.subheader("üí° Suggestions")
            st.write(data["suggestions"])

            # Save to history
            entry = {
                "Timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "Prompt": prompt,
                "Clarity": data["clarity"],
                "Specificity": data["specificity"],
                "Context": data["context"],
                "Creativity": data["creativity"],
                "Relevance": data["relevance"],
                "Ambiguity": data["ambiguity"],
                "Total Score": data["total_score"]
            }
            st.session_state.history.append(entry)

        except Exception as e:
            st.error("Couldn't parse the response. Here's the raw reply:")
            st.code(result)
            st.exception(e)

if st.button("üîß Auto-Improve This Prompt"):
    if not prompt.strip():
        st.warning("Please enter a prompt.")
    else:
        with st.spinner("Thinking..."):
            improved = improve_prompt(prompt)
        st.subheader("‚ú® Improved Prompt")
        st.code(improved, language="markdown")

st.markdown("---")
st.subheader("üÜö Compare Two Prompts")

col1, col2 = st.columns(2)
with col1:
    prompt1 = st.text_area("Prompt 1", key="p1")
with col2:
    prompt2 = st.text_area("Prompt 2", key="p2")

if st.button("Compare Prompts"):
    if not prompt1.strip() or not prompt2.strip():
        st.warning("Please enter both prompts.")
    else:
        with st.spinner("Comparing..."):
            result = compare_prompts(prompt1, prompt2)

try:
    if isinstance(result, str):
        data = json.loads(result)  # Try parsing as JSON
        st.success(f"üèÜ Better Prompt: **Prompt {data['better_prompt']}**")
        st.markdown(f"### ü§ñ Reason")
        st.write(data["justification"])
    elif isinstance(result, dict):
        st.success(f"üèÜ Better Prompt: **Prompt {result['better_prompt']}**")
        st.markdown("### ü§ñ Reason")
        st.write(result["justification"])
    else:
        st.info("üìù Comparison Result")
        st.write(result)
except Exception:
    st.info("üìù Comparison Result (Text Only)")
    st.write(result)


        


st.markdown("---")
st.subheader("üìú Prompt History")

if st.session_state.history:
    df = pd.DataFrame(st.session_state.history)
    st.dataframe(df, use_container_width=True)

    csv = df.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="‚¨áÔ∏è Download History as CSV",
        data=csv,
        file_name='prompt_history.csv',
        mime='text/csv'
    )
else:
    st.info("No prompts evaluated yet.")
